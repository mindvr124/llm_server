# uvicorn llm_server:app --host 0.0.0.0 --port 8000 (--reload : ì„ íƒ. ì½”ë“œ ìˆ˜ì • ì‹œ ìë™ ì„œë²„ ì¬ì‹œì‘)

####################################################################################
# json ì–‘ì‹
# {
#     "model":"gpt-4o",
#     "temperature":"0.3",
#     "system":"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê³µê°í•˜ëŠ” ì‹¬ë¦¬ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸í˜• ì‘ë‹µì„ ì´ì–´ê°€ì£¼ì„¸ìš”.",
#     "user_input":"ì•ˆë…•í•˜ì„¸ìš”"
# }
####################################################################################

from fastapi import FastAPI, WebSocket
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
from dotenv import load_dotenv
import os
import asyncio
import json

load_dotenv()
app = FastAPI()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

def get_streaming_llm(model, temperature, callback):
    api_key = os.getenv("OPENAI_API_KEY")
    print("ğŸ”‘ API í‚¤:", api_key)
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        streaming=True,
        callbacks=[callback],
        openai_api_key=OPENAI_API_KEY
    )

# íˆìŠ¤í† ë¦¬ ì €ì¥ ë³€ìˆ˜ (ì„œë²„ ì‹¤í–‰ ì¤‘ ìœ ì§€ë¨)
chat_history = []

import json

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("ChatOpenAI ì‹¤ì œ í´ë˜ìŠ¤:", ChatOpenAI.__module__)
    print("ğŸ”Œ WebSocket ì—°ê²° ìˆ˜ë½ë¨")

    while True:
        try:
            print("ğŸ“© ë©”ì‹œì§€ ëŒ€ê¸° ì¤‘...")
            data_bytes = await websocket.receive_bytes()
            text = data_bytes.decode("utf-8")
            data = json.loads(text)

            model = data.get("model")
            temperature = data.get("temperature")
            system = data.get("system")
            user_input = data.get("user_input")

            if not system or not user_input:
                await websocket.send_json({"error": "template and user_input are required"})
                continue

            history = "\n".join(
                [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
            )

            template = """
            {system}

            ì´ì „ ëŒ€í™” :
            {history}

            ì‚¬ìš©ì ë©”ì„¸ì§€ :
            {user_input}
            """

            prompt = PromptTemplate(
                input_variables=["system", "history", "user_input"],
                template=template
            )
            print(f"ğŸ“¨ ì‚¬ìš©ì: {user_input}")
            callback = AsyncIteratorCallbackHandler()
            llm = get_streaming_llm(model, temperature, callback)

            chain = prompt | llm

            response = asyncio.create_task(chain.ainvoke({
                "system": system,
                "history": history,
                "user_input": user_input
            }))

            async for chunk in callback.aiter():
                await websocket.send_json({"chunk": chunk})

            response_text = await response
            chat_history.append({
                "user": user_input,
                "response": response_text.content,
            })

            await websocket.send_json({"done": True, "content": response_text.content})
            print(f"ğŸ“¨ ìƒë‹´ì‚¬: {response_text.content}")

        except Exception as e:
            await websocket.send_json({"error": str(e)})
            print("âŒ ì—ëŸ¬ ë°œìƒ:", e)
            break



if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))  # RenderëŠ” PORT í™˜ê²½ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì¤Œ
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port)