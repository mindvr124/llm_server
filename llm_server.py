# uvicorn llm_server:app --host 0.0.0.0 --port 8000 (--reload : ì„ íƒ. ì½”ë“œ ìˆ˜ì • ì‹œ ìë™ ì„œë²„ ì¬ì‹œì‘)

####################################################################################
# json ì–‘ì‹
# {
#     "model":"gpt-4o",
#     "temperature":"0.3",
#     "system":"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê³µê°í•˜ëŠ” ì‹¬ë¦¬ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸í˜• ì‘ë‹µì„ ì´ì–´ê°€ì£¼ì„¸ìš”.",
#     "user_input":"ì•ˆë…•í•˜ì„¸ìš”"
# }
####################################################################################

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
import os
import asyncio
import json

load_dotenv()
app = FastAPI()

# =============================================
# DB ì—°ê²° ì„¤ì •
# =============================================
user = os.getenv("DB_USER")
password = os.getenv("DB_PASSWORD")
host = os.getenv("SERVER_HOST")
port = "3306"
database = os.getenv("DB_NAME")

# ==========================================================================================
# MariaDBì— ìƒë‹´ ë‚´ìš© ì €ì¥
# =========================================================================================
engine = create_engine(
        f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}?charset=utf8mb4"
    )

# ëª¨ë“  ìƒë‹´ ë‚´ìš© ì €ì¥
def save_counsel_history(user_id, user_input, answer):
    query = text("INSERT INTO counsel_history (user_id, user_input, answer) VALUES (:user_id, :user_input, :answer)")
    with engine.begin() as conn:  # ìë™ commit í¬í•¨
        conn.execute(query, {
            "user_id": user_id,
            "user_input": user_input,
            "answer": answer
        })

# ìƒë‹´ ìš”ì•½ ë‚´ìš© ì €ì¥
def save_counsel_summary(user_id, content):
    query = text("INSERT INTO counsel_summary (user_id, content) VALUES (:user_id, :content)")
    with engine.begin() as conn:
        conn.execute(query, {
            "user_id": user_id,
            "content": content
        })

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ë©”ëª¨ë¦¬ êµ¬í˜„, ìš”ì•½ ëŒ€í™” ì €ì¥
chat_history = []
summary_text = ""

def get_streaming_llm(model, temperature, callback):
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        streaming=True,
        callbacks=[callback],
        openai_api_key=OPENAI_API_KEY
    )

# =========================
# ìš”ì•½ ìƒì„± í•¨ìˆ˜
# =========================
def generate_summary(llm, history, summary_text):
    summary_template = PromptTemplate(
        input_variables=["history", "summary_text"],
        template="""
        ë‹¤ìŒì€ í˜„ì¬ ì§„í–‰ì¤‘ì¸ ìƒë‹´ì˜ ì‚¬ìš©ìì™€ ìƒë‹´ì‚¬ì˜ ëŒ€í™”ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì´ë¦„, ìƒë‹´ ì´ìœ ì™€ ê·¸ì— ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ ì¤‘ë³µì´ ì—†ë„ë¡ ìš”ì•½í•´ ì£¼ì„¸ìš”.
        ì´ì „ ìš”ì•½ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•´ì„œ í•¨ê»˜ ìš”ì•½í•´ì£¼ì„¸ìš”.
        ë§ˆë¬´ë¦¬ ì¸ì‚¬ê°€ ë‚˜ì™”ê±°ë‚˜, ìƒë‹´ ì¢…ë£Œ ì˜ì‚¬ë¥¼ ë°í˜”ì„ ë•Œë§Œ ë§ˆì§€ë§‰ì— "[ìƒë‹´ ì¢…ë£Œ]"ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.

        ì´ì „ ìš”ì•½ ëŒ€í™”:
        {summary_text}
        ëŒ€í™”:
        {history}
        """
    )
    summary_chain = summary_template | llm
    return summary_chain.ainvoke({
        "summary_text": summary_text,
        "history": history
    })

# =============================================
# WebSocket ì—°ê²° ì„¤ì •
# =============================================
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("ğŸ”Œ WebSocket ì—°ê²° ìˆ˜ë½ë¨")

    user_id = "test_id"
    print("--------------------------------")
    print("test_id ë¡œ ìƒë‹´ ë‚´ìš©ì´ ì €ì¥ë©ë‹ˆë‹¤.")
    print("--------------------------------")

    global chat_history, summary_text
    while True:
        try:
            print("ğŸ“© ë©”ì‹œì§€ ëŒ€ê¸° ì¤‘...")
            data = await websocket.receive_json()

            model = data.get("model")
            temperature = data.get("temperature")
            system = data.get("system")
            user_input = data.get("user_input")

            # if not system or not user_input:
            #     err = json.dumps(
            #         {"error": "systemê³¼ user_input í•„ë“œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."},
            #         ensure_ascii=False
            #     )
            #     await websocket.send_bytes(err.encode("utf-8"))
            #     continue

            # history êµ¬ì„± (ìš”ì•½ + ìµœê·¼ 5í„´)
            if summary_text:
                recent_history = "\n".join(
                [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history[-5:]]
                )
                history = f"ìš”ì•½ëœ ëŒ€í™”: {summary_text}\n\nìµœê·¼ ëŒ€í™”:\n{recent_history}"
            else:
                history = "\n".join(
                    [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
                )

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            template = """
            {system}
            'ìš”ì•½ëœ ëŒ€í™”'ê°€ ë“¤ì–´ì˜¤ë©´ ì¸ì‚¬ë¥¼ í•˜ì§€ ë§ê³  ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°€ì£¼ì„¸ìš”.

            ì´ì „ ëŒ€í™” :
            {history}

            ì‚¬ìš©ì ë©”ì„¸ì§€ :
            {user_input}
            """
            prompt = PromptTemplate(
                input_variables=["system", "history", "user_input"],
                template=template
            )

            print(f"ğŸ“¨ ì‚¬ìš©ì: {user_input}")
            callback = AsyncIteratorCallbackHandler()
            llm = get_streaming_llm(model, temperature, callback)
            chain = prompt | llm

            response = asyncio.create_task(chain.ainvoke({
                "system": system,
                "history": history,
                "user_input": user_input
            }))

            # chunk ì „ì†¡
            # async for chunk in callback.aiter():
            #     chunk_bytes = json.dumps({"chunk": chunk}, ensure_ascii=False).encode("utf-8")
            #     await websocket.send_bytes(chunk_bytes)
            async for chunk in callback.aiter(): # local test line
                await websocket.send_json({"chunk": chunk})  # local test line

            response_text = await response
            chat_history.append({
                "user": user_input,
                "response": response_text.content,
            })

            # DBì— ìƒë‹´ ë‚´ìš© ì €ì¥
            save_counsel_history(user_id, user_input, response_text.content)

            # ìƒë‹´ ë‚´ìš© ìš”ì•½ (ë§¤ 5í„´)
            if len(chat_history) % 5 == 0:
                full_history = "\n".join(
                    [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
                )
                summary_response = await generate_summary(llm, full_history, summary_text)
                summary_text = summary_response.content
                save_counsel_summary(user_id, summary_text)
                print("ğŸ“ ìš”ì•½ ì—…ë°ì´íŠ¸:", summary_text)
                chat_history=[]

            # done_msg = json.dumps(
            #     {"done": True, "content": response_text.content},
            #     ensure_ascii=False
            # )
            # await websocket.send_bytes(done_msg.encode("utf-8"))
            # print(f"ğŸ“¨ ìƒë‹´ì‚¬ ì‘ë‹µ: {response_text.content}")
            await websocket.send_json({"done": True, "content": response_text.content}) # local test line
            print(f"ğŸ“¨ ìƒë‹´ì‚¬ ì‘ë‹µ: {response_text.content}") # local test line

        except WebSocketDisconnect:
            print("ğŸ”Œ í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
            if chat_history:
                full_history = "\n".join(
                    [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
                )
                print("ğŸ§ª ìš”ì•½ ìƒì„± ì‹œì‘")
                summary_response = await generate_summary(llm, full_history)
                summary_text = summary_response.content
                save_counsel_summary(user_id, summary_text)
                print("ğŸ“„ ìš”ì•½ ê²°ê³¼:", summary_response.content)
            break

        except Exception as e:
            print("âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬:", e)
            try:
                # error_msg = json.dumps({"error": str(e)}, ensure_ascii=False)
                # await websocket.send_bytes(error_msg.encode("utf-8"))
                await websocket.send_json({"error": str(e)})  # local test line
            except Exception:
                print("âŒ ì˜ˆì™¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ:", e)
                if chat_history:
                    full_history = "\n".join(
                        [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
                    )
                    summary_response = await generate_summary(llm, full_history)
                    summary_text = summary_response.content
                    save_counsel_summary(user_id, summary_text)
            break

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port)
