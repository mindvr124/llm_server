# uvicorn llm_server:app --host 0.0.0.0 --port 8000 (--reload : ì„ íƒ. ì½”ë“œ ìˆ˜ì • ì‹œ ìë™ ì„œë²„ ì¬ì‹œì‘)

####################################################################################
# json ì–‘ì‹
# {
#     "model":"gpt-4o",
#     "temperature":"0.3",
#     "system":"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê³µê°í•˜ëŠ” ì‹¬ë¦¬ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸í˜• ì‘ë‹µì„ ì´ì–´ê°€ì£¼ì„¸ìš”.",
#     "user_input":"ì•ˆë…•í•˜ì„¸ìš”"
# }
####################################################################################

from fastapi import FastAPI, WebSocket
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
from dotenv import load_dotenv
from starlette.websockets import WebSocketDisconnect
import os
import asyncio
import json

load_dotenv()
app = FastAPI()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

def get_streaming_llm(model, temperature, callback):
    api_key = os.getenv("OPENAI_API_KEY")
    print("ğŸ”‘ API í‚¤:", api_key)
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        streaming=True,
        callbacks=[callback],
        openai_api_key=OPENAI_API_KEY
    )

# íˆìŠ¤í† ë¦¬ ì €ì¥ ë³€ìˆ˜ (ì„œë²„ ì‹¤í–‰ ì¤‘ ìœ ì§€ë¨)
chat_history = []

import json

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("ChatOpenAI ì‹¤ì œ í´ë˜ìŠ¤:", ChatOpenAI.__module__)
    print("ğŸ”Œ WebSocket ì—°ê²° ìˆ˜ë½ë¨")

    while True:
        try:
            print("ğŸ“© ë©”ì‹œì§€ ëŒ€ê¸° ì¤‘...")
            text = await websocket.receive_text()
            #text = data_bytes.decode("utf-8")
            data = json.loads(text)

            model = data.get("model")
            temperature = data.get("temperature")
            system = data.get("system")
            user_input = data.get("user_input")

            print(f"ëª¨ë¸: {model}, ì°½ì˜ì„±: {temperature}, ì‹œìŠ¤í…œ: {system}, ì‚¬ìš©ì ì…ë ¥: {user_input}")

            if not system or not user_input:
                err = json.dumps(
                    {"error": "systemê³¼ user_input í•„ë“œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."},
                    ensure_ascii=False
                )
                await websocket.send_bytes(err.encode("utf-8"))
                continue

            history = "\n".join(
                [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
            )

            template = """
            {system}

            ì´ì „ ëŒ€í™” :
            {history}

            ì‚¬ìš©ì ë©”ì„¸ì§€ :
            {user_input}
            """

            prompt = PromptTemplate(
                input_variables=["system", "history", "user_input"],
                template=template
            )
            print(f"ğŸ“¨ ì‚¬ìš©ì: {user_input}")
            callback = AsyncIteratorCallbackHandler()
            llm = get_streaming_llm(model, temperature, callback)

            chain = prompt | llm

            response = asyncio.create_task(chain.ainvoke({
                "system": system,
                "history": history,
                "user_input": user_input
            }))

             # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ì†¡ (chunk ë‹¨ìœ„)
            async for chunk in callback.aiter():
                chunk_bytes = json.dumps({"chunk": chunk}, ensure_ascii=False).encode("utf-8")
                await websocket.send_bytes(chunk_bytes)

            # ìµœì¢… ê²°ê³¼ ì²˜ë¦¬
            response_text = await response
            chat_history.append({
                "user": user_input,
                "response": response_text.content,
            })
    
            done_msg = json.dumps(
                {"done": True, "content": response_text.content},
                ensure_ascii=False
            )
            await websocket.send_bytes(done_msg.encode("utf-8"))
            print(f"ğŸ“¨ ìƒë‹´ì‚¬ ì‘ë‹µ: {response_text.content}")

        except WebSocketDisconnect:
            print("ğŸ”Œ í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
            break
        
        except Exception as e:
            print("âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬:", e)
            try:
                error_msg = json.dumps({"error": str(e)}, ensure_ascii=False)
                await websocket.send_bytes(error_msg.encode("utf-8"))
            except Exception:
                print("âŒ ì˜ˆì™¸ ë°œìƒ:", e)
            break

       


if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))  # RenderëŠ” PORT í™˜ê²½ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì¤Œ
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port)