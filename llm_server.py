# uvicorn llm_server:app --host 0.0.0.0 --port 8000 (--reload : ì„ íƒ. ì½”ë“œ ìˆ˜ì • ì‹œ ìë™ ì„œë²„ ì¬ì‹œì‘)

####################################################################################
# json ì–‘ì‹
# {
#     "model":"gpt-4o",
#     "temperature":"0.3",
#     "system":"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê³µê°í•˜ëŠ” ì‹¬ë¦¬ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸í˜• ì‘ë‹µì„ ì´ì–´ê°€ì£¼ì„¸ìš”.",
#     "user_input":"ì•ˆë…•í•˜ì„¸ìš”"
# }
####################################################################################

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
import os
import asyncio
import uvicorn

load_dotenv()
app = FastAPI()

# =============================================
# DB ì—°ê²° ì„¤ì •
# =============================================
user = os.getenv("DB_USER")
password = os.getenv("DB_PASSWORD")
host = os.getenv("SERVER_HOST")
port = "3306"
database = os.getenv("DB_NAME")

# =============================================
# MariaDBì— ìƒë‹´ ë‚´ìš© ì €ì¥
# =============================================
engine = create_engine(
        f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}?charset=utf8mb4"
    )

# =============================================
# ì‚¬ìš©ìê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
# =============================================
def ensure_user_exists(user_id):
    check_query = text("SELECT COUNT(*) FROM user WHERE user_id = :user_id")
    with engine.begin() as conn:
        result = conn.execute(check_query, {"user_id": user_id})
        count = result.scalar()
        print(f"ì‚¬ìš©ì ID : {user_id}")

        # ì‚¬ìš©ìê°€ ì—†ìœ¼ë©´ ìƒì„±
        if count == 0:
            insert_user_query = text("INSERT INTO user (user_id) VALUES (:user_id)")
            conn.execute(insert_user_query, {
                "user_id": user_id
            })
            print(f"ìƒˆ ì‚¬ìš©ì ìƒì„±: {user_id}")

# =============================================
# ëª¨ë“  ìƒë‹´ ë‚´ìš© ì €ì¥
# =============================================
def save_counsel_history(user_id, user_input, answer):
    query = text("INSERT INTO counsel_history (user_id, user_input, answer) VALUES (:user_id, :user_input, :answer)")
    with engine.begin() as conn:  # ìë™ commit í¬í•¨
        conn.execute(query, {
            "user_id": user_id,
            "user_input": user_input,
            "answer": answer
        })

# =============================================
# ìš”ì•½ ìƒì„± í•¨ìˆ˜
# =============================================
def generate_summary(llm, history, summary_text):
    summary_template = PromptTemplate(
        input_variables=["history", "summary_text"],
        template="""
        ì´ì „ ìš”ì•½ ëŒ€í™”:
        {summary_text}

        -----------------
        ë‹¤ìŒì€ í˜„ì¬ ì§„í–‰ì¤‘ì¸ ìƒë‹´ì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìì˜ ì´ë¦„, ìƒë‹´ ì´ìœ ì™€ ê·¸ì— ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ ì¤‘ë³µì´ ì—†ë„ë¡ ìš”ì•½í•´ ì£¼ì„¸ìš”.
        ì´ì „ ìš”ì•½ ëŒ€í™”ì—ì„œ ì¤‘ìš”í•œ ë‚´ìš©ì€ ì‚­ì œí•˜ì§€ ë§ê³  ë§ë¶™ì—¬ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”.

        ë¬´ì˜ë¯¸í•œ ëŒ€í™”ë‚˜ ì¸ì‚¬ë§Œ ìˆëŠ” ê²½ìš° ìš”ì•½í•˜ì§€ ë§ê³  ë°›ì€ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë³´ë‚´ì£¼ì„¸ìš”.
        -----------------

        ì‹¤ì‹œê°„ ëŒ€í™”:
        {history}
        """
    )
    summary_chain = summary_template | llm
    return summary_chain.ainvoke({
        "summary_text": summary_text,
        "history": history
    })

# =============================================
# ìƒë‹´ ìš”ì•½ ë‚´ìš© ì €ì¥
# =============================================
def save_counsel_summary(user_id, content):
    if len(content) < 30:
        return 0
    query = text("INSERT INTO counsel_summary (user_id, content) VALUES (:user_id, :content)")
    with engine.begin() as conn:
        conn.execute(query, {
            "user_id": user_id,
            "content": content
        })

# =============================================
# ì§€ë‚œ ìƒë‹´ ìš”ì•½ ë‚´ìš© ì¡°íšŒ
# =============================================
def get_counsel_summary(user_id):
    query = text("""
        SELECT content 
        FROM counsel_summary 
        WHERE user_id = :user_id 
        ORDER BY id DESC 
        LIMIT 1
    """)
    with engine.begin() as conn:
        result = conn.execute(query, {"user_id": user_id})
        row = result.fetchone()
        return row[0] if row else None

# =============================================
# ìŠ¤íŠ¸ë¦¬ë° ëª¨ë¸ ìƒì„±
# =============================================
def get_streaming_llm(model, temperature, callback):
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        streaming=True,
        callbacks=[callback],
        openai_api_key=OPENAI_API_KEY
    )

# =============================================
# ì‚¬ìš©ì ì´ë¦„ ì¶”ì¶œ í•¨ìˆ˜
# =============================================
async def extract_user_name(llm, history, user_id):
    check_query = text("SELECT user_name FROM user WHERE user_id = :user_id")
    with engine.begin() as conn:
        result = conn.execute(check_query, {"user_id": user_id})
        row = result.fetchone()

        if row and row[0] not in (None, "", "ì•Œìˆ˜ì—†ìŒ"):
            # ì´ë¯¸ ì´ë¦„ì´ ìˆìœ¼ë©´ ì¶”ì¶œí•˜ì§€ ì•ŠìŒ
            return None

    # ì´ë¦„ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
    name_prompt = PromptTemplate(
        input_variables=["history"],
        template="""
        ë‹¤ìŒì€ ì‚¬ìš©ìì™€ ìƒë‹´ì‚¬ì˜ ëŒ€í™”ì…ë‹ˆë‹¤.

        ì´ ëŒ€í™”ì—ì„œ **ì‚¬ìš©ìì˜ ì´ë¦„**ë§Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
        - "ì €ëŠ” ê¹€ì² ìˆ˜ì˜ˆìš”", "ì œ ì´ë¦„ì€ ì •ì€ì§€ì…ë‹ˆë‹¤", "ì€ì§€ë¼ê³  ë¶ˆëŸ¬ì£¼ì„¸ìš”", "ì¤€í˜¸ë‹˜,"ê³¼ ê°™ì€ í‘œí˜„ì„ ì°¸ê³ í•˜ì„¸ìš”.
        - ë°˜ë“œì‹œ **ì´ë¦„ë§Œ** ì¶œë ¥í•˜ì„¸ìš”. ì´ë¦„ì´ ì—†ëŠ” ê²½ìš° "ì•Œìˆ˜ì—†ìŒ"ì´ë¼ê³  ì¶œë ¥í•˜ì„¸ìš”.
        - ì˜ˆì‹œ ì¶œë ¥: ê¹€ì² ìˆ˜ / ì€ì§€ / ì•Œìˆ˜ì—†ìŒ

        ëŒ€í™”:
        {history}

        ì‚¬ìš©ì ì´ë¦„:
        """
    )
    chain = name_prompt | llm
    user_name = (await chain.ainvoke({"history": history})).content.strip()

    # ê¸°ì¡´ ìœ ì €ë¼ë„ ì´ë¦„ ì—…ë°ì´íŠ¸
    update_query = text("UPDATE user SET user_name = :user_name WHERE user_id = :user_id")
    with engine.begin() as conn:
        conn.execute(update_query, {"user_id": user_id, "user_name": user_name})

    return user_name



# =============================================
# WebSocket ì—°ê²° ì„¤ì •
# =============================================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ë©”ëª¨ë¦¬ êµ¬í˜„, ìš”ì•½ ëŒ€í™” ì €ì¥
chat_history = []
summary_text = ""

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("ğŸ”Œ WebSocket ì—°ê²° ìˆ˜ë½ë¨")

    global chat_history, summary_text
    while True:
        try:
            print("ğŸ“© ë©”ì‹œì§€ ëŒ€ê¸° ì¤‘...")
            data = await websocket.receive_json()

            user_id = data.get("user_id")
            ensure_user_exists(user_id)
            print(f"USER_ID: {user_id}")
            print("------------------------")

            model = data.get("model")
            temperature = data.get("temperature")
            system = data.get("system")
            user_input = data.get("user_input")

            # if not system or not user_input:
            #     err = json.dumps(
            #         {"error": "systemê³¼ user_input í•„ë“œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."},
            #         ensure_ascii=False
            #     )
            #     await websocket.send_bytes(err.encode("utf-8"))
            #     continue

            # ì§€ë‚œ ìƒë‹´ ìš”ì•½ ë‚´ìš© ì¡°íšŒ
            summary_data = get_counsel_summary(user_id)
            print(f"ì§€ë‚œ ìƒë‹´ ìš”ì•½ ë‚´ìš© : {summary_data}")

            # history êµ¬ì„± (ìš”ì•½ + ìµœê·¼ 5í„´)
            if summary_text:
                recent_history = "\n".join(
                [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history[-5:]]
                )
                history = f"ìš”ì•½ëœ ëŒ€í™”: {summary_text}\n\nìµœê·¼ ëŒ€í™”:\n{recent_history}"
            else:
                history = "\n".join(
                    [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
                )

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            if(summary_data):
                template = """
                    {system}

                    'ìš”ì•½ ëŒ€í™”'ëŠ” ì‹¤ì‹œê°„ ìš”ì•½ì¤‘ì¸ ëŒ€í™”ì…ë‹ˆë‹¤. ì¸ì‚¬ë¥¼ í•˜ì§€ ë§ê³  ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°€ì£¼ì„¸ìš”.
                    'ì§€ë‚œ ìƒë‹´ ëŒ€í™”'ëŠ” ì§€ë‚œ ìƒë‹´ ìš”ì•½ ë‚´ìš©ì…ë‹ˆë‹¤. ì–´ë–¤ ë³€í™”ê°€ ìˆì—ˆëŠ”ì§€ ë¬¼ì–´ë³´ë©° ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°€ì£¼ì„¸ìš”.

                    ì§€ë‚œ ìƒë‹´ ëŒ€í™” :
                    {summary_data}

                    ìš”ì•½ ëŒ€í™” :
                    {history}

                    ì‚¬ìš©ì ë©”ì„¸ì§€ :
                    {user_input}
                """
                prompt = PromptTemplate(
                    input_variables=["system", "summary_data", "history", "user_input"],
                    template=template
                )
                print(f"ğŸ“¨ ì‚¬ìš©ì: {user_input}")
                callback = AsyncIteratorCallbackHandler()
                llm = get_streaming_llm(model, temperature, callback)
                chain = prompt | llm

                response = asyncio.create_task(chain.ainvoke({
                    "system": system,
                    "summary_data": summary_data,
                    "history": history,
                    "user_input": user_input
                }))

            # ì²« ìƒë‹´ì¸ ì‚¬ìš©ì
            else: 
                template = """
                {system}
                'ìš”ì•½ ëŒ€í™”'ëŠ” ì‹¤ì‹œê°„ ìš”ì•½ì¤‘ì¸ ëŒ€í™”ì…ë‹ˆë‹¤. ì¸ì‚¬ë¥¼ í•˜ì§€ ë§ê³  ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°€ì£¼ì„¸ìš”.

                ìš”ì•½ ëŒ€í™” :
                {history}

                ì‚¬ìš©ì ë©”ì„¸ì§€ :
                {user_input}
                """
                prompt = PromptTemplate(
                    input_variables=["system", "history", "user_input"],
                    template=template
                )

                print(f"ğŸ“¨ ì‚¬ìš©ì: {user_input}")
                callback = AsyncIteratorCallbackHandler()
                llm = get_streaming_llm(model, temperature, callback)
                chain = prompt | llm

                response = asyncio.create_task(chain.ainvoke({
                    "system": system,
                    "history": history,
                    "user_input": user_input
                }))

            # chunk ì „ì†¡
            # async for chunk in callback.aiter():
            #     chunk_bytes = json.dumps({"chunk": chunk}, ensure_ascii=False).encode("utf-8")
            #     await websocket.send_bytes(chunk_bytes)
            async for chunk in callback.aiter(): # local test line
                await websocket.send_json({"chunk": chunk})  # local test line

            response_text = await response
            chat_history.append({
                "user": user_input,
                "response": response_text.content,
            })

            # DBì— ìƒë‹´ ë‚´ìš© ì €ì¥
            save_counsel_history(user_id, user_input, response_text.content)

            # ìƒë‹´ ë‚´ìš© ìš”ì•½ (ë§¤ 5í„´)
            if len(chat_history) % 5 == 0:
                full_history = "\n".join(
                    [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
                )
                summary_response = await generate_summary(llm, full_history, summary_text)
                summary_text = summary_response.content
                print("ğŸ“ ìš”ì•½ ì—…ë°ì´íŠ¸:", summary_text)
                chat_history=[]

            # done_msg = json.dumps(
            #     {"done": True, "content": response_text.content},
            #     ensure_ascii=False
            # )
            # await websocket.send_bytes(done_msg.encode("utf-8"))
            # print(f"ğŸ“¨ ìƒë‹´ì‚¬ ì‘ë‹µ: {response_text.content}")
            await websocket.send_json({"done": True}) # local test line
            print(f"ğŸ“¨ ìƒë‹´ì‚¬ ì‘ë‹µ: {response_text.content}") # local test line

        except WebSocketDisconnect:
            print("ğŸ”Œ í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
            if chat_history:
                full_history = "\n".join(
                    [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
                )
                print("ğŸ§ª ìš”ì•½ ìƒì„± ì‹œì‘")
                summary_response = await generate_summary(llm, full_history, summary_text)
                summary_text = summary_response.content
                save_counsel_summary(user_id, summary_text)
                await extract_user_name(llm, summary_text, user_id)
                print("ğŸ“„ ìš”ì•½ ê²°ê³¼:", summary_response.content)
            break

        except Exception as e:
            print("âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬:", e)
            try:
                # error_msg = json.dumps({"error": str(e)}, ensure_ascii=False)
                # await websocket.send_bytes(error_msg.encode("utf-8"))
                await websocket.send_json({"error": str(e)})  # local test line
            except Exception:
                print("âŒ ì˜ˆì™¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ:", e)
                if chat_history:
                    full_history = "\n".join(
                        [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
                    )
                    summary_response = await generate_summary(llm, full_history, summary_text)
                    summary_text = summary_response.content
                    save_counsel_summary(user_id, summary_text)
            break

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port)
