# uvicorn llm_server:app --host 0.0.0.0 --port 8000 (--reload : ì„ íƒ. ì½”ë“œ ìˆ˜ì • ì‹œ ìë™ ì„œë²„ ì¬ì‹œì‘)

####################################################################################
# json ì–‘ì‹
# {
#     "model":"gpt-4o",
#     "temperature":"0.3",
#     "system":"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê³µê°í•˜ëŠ” ì‹¬ë¦¬ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸í˜• ì‘ë‹µì„ ì´ì–´ê°€ì£¼ì„¸ìš”.",
#     "user_input":"ì•ˆë…•í•˜ì„¸ìš”"
# }
####################################################################################

from fastapi import FastAPI, WebSocket
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
from dotenv import load_dotenv
import os
import asyncio

load_dotenv()
app = FastAPI()

def get_streaming_llm(model, temperature, callback):
    api_key = os.getenv("OPENAI_API_KEY")
    print("ğŸ”‘ API í‚¤:", api_key)
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        streaming=True,
        callbacks=[callback],
        openai_api_key=api_key  # âœ… ì´ ì¤„ì´ ê¼­ ìˆì–´ì•¼ í•¨
    )

# íˆìŠ¤í† ë¦¬ ì €ì¥ ë³€ìˆ˜ (ì„œë²„ ì‹¤í–‰ ì¤‘ ìœ ì§€ë¨)
chat_history = []

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("ChatOpenAI ì‹¤ì œ í´ë˜ìŠ¤:", ChatOpenAI.__module__)
    print("ğŸ”Œ WebSocket ì—°ê²° ìˆ˜ë½ë¨")

    while True:
        try:
            print("ğŸ“© ë©”ì‹œì§€ ëŒ€ê¸° ì¤‘...")
            data = await websocket.receive_json()

            model = data.get("model")
            temperature = data.get("temperature")
            system = data.get("system")
            user_input = data.get("user_input")
            

            if not system or not user_input:
                await websocket.send_json({"error": "template and user_input are required"})
                continue
            
            history = "\n".join(
                [f"ì‚¬ìš©ì: {item['user']}\nìƒë‹´ì‚¬: {item['response']}" for item in chat_history]
            )

            template = """
            {system}

            ì´ì „ ëŒ€í™” :
            {history}

            ì‚¬ìš©ì ë©”ì„¸ì§€ :
            {user_input}
            """

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt = PromptTemplate(
                input_variables=["system", "history", "user_input"],
                template=template
            )
            print(f"ğŸ“¨ ì‚¬ìš©ì: {user_input}")
            callback = AsyncIteratorCallbackHandler()
            llm = get_streaming_llm(model, temperature, callback)

            # ìµœì‹  LangChain êµ¬ì¡°: RunnableSequence
            chain = prompt | llm

            # Streaming ì‹¤í–‰
            response = asyncio.create_task(chain.ainvoke({
                "system": system,
                "history": history,
                "user_input": user_input
            }))

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ì†¡
            async for chunk in callback.aiter():
                await websocket.send_json({"chunk": chunk})

            # ê²°ê³¼ ëŒ€ê¸°
            response_text = await response
            # ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì €ì¥
            chat_history.append({
                "user": user_input,
                "response": response_text.content,
            })

            await websocket.send_json({"done": True, "content":response_text.content})
            print(f"ğŸ“¨ ìƒë‹´ì‚¬: {response_text.content}")
        except Exception as e:
            await websocket.send_json({"error": str(e)})
            print("âŒ ì—ëŸ¬ ë°œìƒ:", e)
            break
